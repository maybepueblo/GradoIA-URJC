>[!NOTE]
>Necesitamos la función de activación para otorgar flexibilidad a nuestro modelo. Si esta no existiese, tendríamos menos parámetros, siendo equiparable a un modelo lineal, que es peor que nuestra red de neuronas.

unidades ocultas
$h_1 = a[\theta_{10}+\theta_{11}x_1+\theta_{12}x_2]$  siendo $0_{10}$ el sesgo asociado
## Teorema de aproximación universal