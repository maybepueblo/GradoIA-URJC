{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74d8939f-b109-4e8c-8b56-8d531900b31a",
   "metadata": {},
   "source": [
    "#  Entregable 3 - Aprendizaje Automático II\n",
    "\n",
    "\n",
    "***<p style=\"text-align:center;\">Implementación de una MLP con Backpropagation</p>***\n",
    "\n",
    "En este cuaderno vamos a implementar una MLP (Multi-Layer Perceptron) con Backpropagation haciendo uso exclusivo de `numpy`.\n",
    "\n",
    "### Normas de Entrega\n",
    "\n",
    "1. El formato de entrega será en una carpeta comprimida con nombre: {Iniciales de Nombre y Apellidos}_E3.zip, en Aula Virtual en la fecha señalada en la plataforma y comunicada en clase previamente.\n",
    "    * Por ejemplo: Iván Ramírez Díaz ==> `IRD_E3.zip`\n",
    "2. El contenido de dicha carpeta será:\n",
    "    * Obligatorio: Notebook relleno del Entregable 3.\n",
    "    * Opcional: Memoria (pdf), en caso de necesitar dar alguna explicación.\n",
    "3. Antes de la entrega, se debe comprobar que el código completo funciona.\n",
    "4. La entrega es individual.\n",
    "\n",
    "### Evaluación\n",
    "\n",
    "La práctica entregable tiene un peso global de 1/4 puntos (los 4 entregables son el 10% de la nota final).\n",
    "\n",
    "La práctica entregable se calificará sobre 10 puntos. Las puntuaciones son las siguientes:\n",
    "\n",
    "- **[Ejercicio 1]** Implementa las clases: `Linear`, `Sigmoid` y `MSELoss`. (1.5 puntos) \n",
    "- **[Ejercicio 2]** Implementa la clase `SGD`y el modelo `MLP`. (1.5 puntos)\n",
    "- **[Ejercicio 3]**  Entrena con los datos propuestos, una MLP en regresión. (2 puntos)\n",
    "- **[Ejercicio 4]** Implementa la clase `L1Loss` y entrena el mismo modelo ahora con esta función de pérdidas. Compara los resultados. (1.5 puntos)\n",
    "- **[Ejercicio 5]** Entrena el modelo anterior adaptándolo al `breast cancer dataset`que ya conoces usando `MSELoss` y `L1Loss`. De nuevo, compara los resultados. (1.5 puntos)\n",
    "- **[Ejercicio 6]** Entrena el modelo anterior adaptándolo al `breast cancer dataset`que ya conoces implementando previamente una función de pérdida de clasificación, por ejemplo, la *binary cross entropy*  `BXELoss`. (2 puntos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1e7d1d-5efd-4e1d-aac7-674becab7c43",
   "metadata": {},
   "source": [
    "## **[Ejercicio 1]**\n",
    "\n",
    "Implementa las clases: `Linear`, `Sigmoid` y `MSELoss`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc77d317-0f06-4721-a96c-4e8805cb40d9",
   "metadata": {},
   "source": [
    "### Definición de clase base\n",
    "\n",
    "Como hemos visto en clase, todas y cada una de las funciones en un framework de diferenciación automática implementan tanto la función en sí como sus derivadas. De esta manera, se habilita el uso del algoritmo de retropropagación.\n",
    "\n",
    "Toda clase que implemente una función en una red neuronal, tendrá dos métodos fundamentales:\n",
    "* forward: consiste en la evaluación de dicha función $f(x)$ para una entrada $x$ dada.\n",
    "* backward: consiste en la retropropagación de las derivadas/jacobianos desde la salida hasta los parámetros\n",
    "\n",
    "De forma genérica, vamos a definir unos atributos y métodos mínimos para cada función *diferenciable*:\n",
    "* Si la función no es paramétrica (función de activación, por ejemplo):\n",
    "\n",
    "    * `init`: la clase tendrá `self.p = None` (no tiene parámetros)\n",
    "    * `foward`:\n",
    "      1. El método recibirá: $x$, que será la salida de la función anterior.\n",
    "      2. El método tendrá: `self.f`, que será la evaluación de la función $f(x)$.\n",
    "      3. El método devolverá: `self.f`.\n",
    "    * `backward`:\n",
    "        1. El método recibirá: `dL_df`, que será la derivada de la pérdida hasta la salida de la función $f$.\n",
    "        2. El método tendrá: `self.df_dx`, que será la derivada de la función $f$ con respecto a la entrada $x$.\n",
    "        3. El método devolverá: `dL_dx`, que será la aplicación de la regla de la cadena $\\frac{\\partial L}{\\partial x} = \\frac{\\partial f}{\\partial x} \\frac{\\partial L}{\\partial f}$\n",
    "\n",
    "* Si, además, la función depende de una serie de parámetros:\n",
    "\n",
    "    * `init`:\n",
    "      1. La clase definirá en `self.p` una lista de parámetros a utilizar.\n",
    "      2. La clase tendrá `self.dL_dp` que será una lista de las posibles evaluaciones de los gradientes de los parámetros.\n",
    "    * `backward`:\n",
    "      1. Se calculará además `df_dp`, es decir, las derivadas de $f$ con respecto a cada uno de los parámetros $\\frac{\\partial f}{\\partial p_i}$.\n",
    "      2. La clase calculará `dL_dp` aplicando la regla de la cadena y añadirá la evaluación a la lista `self.dL_dp`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c128591f-4d4d-4bae-b7b9-8d0f28666ae8",
   "metadata": {},
   "source": [
    "### Capa Linear (afín):\n",
    "\n",
    "Siguiendo las indicaciones anteriores, consideramos una capa lineal definida tal que:\n",
    "\n",
    "$$\n",
    "f(x) = {\\bf{W}} {\\bf{x}} + {\\bf{b}}\n",
    "$$\n",
    "\n",
    "donde ${\\bf{W}} \\in \\mathbb{R}^{m,n}, {\\bf{x}} \\in \\mathbb{R}^{n}, {\\bf{b}} \\in \\mathbb{R}^{m}, f \\in \\mathbb{R}^{m}$.\n",
    "\n",
    "Las matrices jacobianas de interés son:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial {\\bf{x}}} = {\\bf{W}}^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial {\\bf{b}}} = {\\bf{I}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial {\\bf{W}}} = \\begin{bmatrix}{\\bf{x}} & \\cdots & 0 \\\\\n",
    "                                                        \\vdots & \\ddots & 0 \\\\\n",
    "                                                        0 & 0 &{\\bf{x}} \\end{bmatrix} \\in \\mathbb{R}^{m\\cdot n, m}\n",
    "$$\n",
    "\n",
    "Sobre esta última matriz jacobiana, conviene hacer un comentario de implementación. En la formulación propuesta arriba, se asume ${\\bf{W}}$ como un vector $\\mathbb{R}^{m\\cdot n}$, aunque en realidad es una matriz de tamaño $m \\times n$. De esta manera, generalizan las derivadas y se pueden ir aplicando los jacobianos siempre de la misma manera, es decir, multiplicando por la izquierda a los gradientes previos. Esto es, de forma genérica: $\\frac{\\partial L}{\\partial x} = \\frac{\\partial f}{\\partial x} \\frac{\\partial L}{\\partial f}$.\n",
    "\n",
    "Sin embargo, esta matriz tiene muchos elementos a 0, lo cual no es eficiente en términos de memoria y operaciones a realizar en la multiplicación matricial. Una alternativa habitual (con abuso de notación) es definir esta derivada (no el Jacobiano) como:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial {\\bf{W}}} = x^T\n",
    "$$\n",
    "\n",
    "y, en vez de multiplicar por la izquierda, hacerlo por la derecha. Es decir, de forma genérica: $\\frac{\\partial L}{\\partial x} =  \\frac{\\partial L}{\\partial f} \\frac{\\partial f}{\\partial x}$. \n",
    "\n",
    "La ventaja es doble: 1) eliminamos las componentes a 0 y las operaciones innecesarias y 2) el resultado es una matriz que coincide en dimensiones con las de ${\\bf{W}}$, lo cual es muy conveniente para la actualización de parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fb59c5-63ea-4a52-aac5-2fcd76908bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7ee54b-8402-4494-a66c-ea7e33e7bf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, input_dim, output_dim, std=1):\n",
    "        # Inicializamos pesos aleatorios y sesgo\n",
    "        self.W = std*np.random.randn(output_dim, input_dim)\n",
    "        self.b = std*np.random.randn(output_dim, 1)\n",
    "\n",
    "        ########################### COMPLETAR ###########################\n",
    "        self.p = \n",
    "        self.dL_dp = \n",
    "        ######################### FIN COMPLETAR #########################\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ########################### COMPLETAR ###########################\n",
    "        self.x =\n",
    "        return   # Producto matricial\n",
    "        ######################### FIN COMPLETAR #########################\n",
    "\n",
    "    def backward(self, dL_df):\n",
    "        ########################### COMPLETAR ###########################\n",
    "        # Derivadas con respecto a la entrada x\n",
    "        self.df_dx = \n",
    "        dL_dx = \n",
    "        self.dL_df = \n",
    "\n",
    "        # Derivadas con respecto a los parámetros\n",
    "\n",
    "        # Con respecto a W\n",
    "        df_dW = \n",
    "        self.dL_dW = \n",
    "\n",
    "        # Con respecto a b\n",
    "        df_db = \n",
    "        self.dL_db = \n",
    "\n",
    "        # Se añaden a la lista de gradientes\n",
    "        self.dL_dp.append([self.dL_dW, self.dL_db])\n",
    "\n",
    "        return self.dL_df\n",
    "        ######################### FIN COMPLETAR #########################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e384b65-41b2-4209-8019-4e082f17541f",
   "metadata": {},
   "source": [
    "### Capa de Activación (sigmoide):\n",
    "\n",
    "Siguiendo las indicaciones anteriores, consideramos una capa de activación definida por una sigmoide:\n",
    "\n",
    "$$\n",
    "f(x) = \\sigma({\\bf{x}}) = \\frac{1}{1 + e^{-{\\bf{x}}}}\n",
    "$$\n",
    "\n",
    "donde ${\\bf{x}} \\in \\mathbb{R}^{n}, f \\in \\mathbb{R}^{n}$.\n",
    "\n",
    "La derivada con respecto a la entrada es:\n",
    "\n",
    "$$\n",
    "\\frac{d f}{d {\\bf{x}}} = \\sigma({\\bf{x}}) \\cdot (1 - \\sigma({\\bf{x}})) = \\begin{bmatrix}\\sigma({\\bf{x}_1}) \\cdot (1 - \\sigma({\\bf{x}_1}))\\\\\n",
    "                                                        \\vdots  \\\\\n",
    "                                                        \\sigma({\\bf{x}_n}) \\cdot (1 - \\sigma({\\bf{x}_n})) \\end{bmatrix} \\in \\mathbb{R}^{n}\n",
    "$$\n",
    "La matriz jacobiana:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial {\\bf{x}}} = diag(\\sigma({\\bf{x}}) \\cdot (1 - \\sigma({\\bf{x}}))) = \\begin{bmatrix}\\sigma({\\bf{x}_1}) \\cdot (1 - \\sigma({\\bf{x}_1})) & \\cdots & 0 \\\\\n",
    "                                                        \\vdots & \\ddots & 0 \\\\\n",
    "                                                        0 & 0 &\\sigma({\\bf{x}_n}) \\cdot (1 - \\sigma({\\bf{x}_n})) \\end{bmatrix} \\in \\mathbb{R}^{ n,n}\n",
    "$$\n",
    "\n",
    "Similar a lo que ocurría antes con el parámetro ${\\bf{W}}$, la matriz Jacobiana de la activación es diagonal, teniendo de nuevo gran cantidad de ceros e incrementando la complejidad del número de operaciones. Por ello, se suele utilizar la derivada $\\frac{d f}{d {\\bf{x}}}$ aplicada como producto de Hadamard, es decir, punto a punto:  $\\frac{\\partial L}{\\partial x} = \\frac{d f}{d {\\bf{x}}} \\odot \\frac{\\partial L}{\\partial f}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2a51e1-c785-4c29-a037-aa1f63aa9906",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        ########################### COMPLETAR ###########################\n",
    "        self.p =\n",
    "        ######################### FIN COMPLETAR #########################\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ########################### COMPLETAR ###########################\n",
    "        self.f =\n",
    "        return self.f\n",
    "        ######################### FIN COMPLETAR #########################\n",
    "\n",
    "    def backward(self, dL_df):\n",
    "        ########################### COMPLETAR ###########################\n",
    "        df_dx = \n",
    "\n",
    "        # Derivada con respecto a la entrada\n",
    "        self.dL_df = \n",
    "        return self.dL_df\n",
    "        ######################### FIN COMPLETAR #########################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5b25af-3902-4207-9e0d-526bf9968faf",
   "metadata": {},
   "source": [
    "### Función de Pérdida (MSE):\n",
    "\n",
    "De forma similar, implementamos la función de pérdida MSE, con una diferencia.\n",
    "\n",
    "Las funciones de pérdida, suelen corresponder al último nodo del grafo de operaciones, por lo que debe iniciar la cadena de retropropagación al \"llamar\" al método `backward` para un modelo dado como argumento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f971cf-eab0-4742-9d6d-bc53b5f243ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "    def __init__(self):\n",
    "        ########################### COMPLETAR ###########################\n",
    "        self.p = \n",
    "        ######################### FIN COMPLETAR #########################\n",
    "\n",
    "    def __call__(self, y_pred, y_true):\n",
    "        return self.forward(y_pred, y_true)\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        ########################### COMPLETAR ###########################\n",
    "        self.y_pred = \n",
    "        self.y_true = \n",
    "        self.f =    \n",
    "        return self.f\n",
    "        ######################### FIN COMPLETAR #########################\n",
    "\n",
    "    def backward(self, model=None):\n",
    "        ########################### COMPLETAR ###########################\n",
    "        self.df_dx = \n",
    "        \n",
    "        if model is not None:\n",
    "            model.backward(self.df_dx)\n",
    "        ######################### FIN COMPLETAR #########################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd1d686-677d-4fb9-a7ad-b4dd01873e73",
   "metadata": {},
   "source": [
    "## **[Ejercicio 2]**\n",
    "\n",
    "Implementa la clase: `SGD`y el modelo `MLP`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8728d45e-631b-4549-a209-18dfbe4acd68",
   "metadata": {},
   "source": [
    "### Modelo de perceptrón multi-capa (MLP):\n",
    "\n",
    "En esta clase, se instancian en el `init`, las distintas capas a utilizar en el modelo, así como sus parámetros (dimensiones de entrada y salida).\n",
    "\n",
    "Además, del `forward`y `backward`, es conveniente añadir un método para resetear (poner a cero o null) los gradientes. Para ello, se añade `zero_grads`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b10579-7175-463d-bfc7-046e9201f88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        ########################### COMPLETAR ###########################\n",
    "        F1 =  # Capa lineal con 128 neuronas de salida\n",
    "        Act1 = # Capa de activación\n",
    "        F2 =  # Capa de salida\n",
    "        self.layers = [F1, Act1, F2]\n",
    "        ######################### FIN COMPLETAR #########################\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ########################### COMPLETAR ###########################\n",
    "        # Forward de la red\n",
    "        \n",
    "        return \n",
    "        ######################### FIN COMPLETAR #########################\n",
    "\n",
    "    def backward(self, dL_df):\n",
    "        ########################### COMPLETAR ###########################\n",
    "        # Backward de la red\n",
    "        \n",
    "        ######################### FIN COMPLETAR #########################\n",
    "\n",
    "    def zero_grads(self):\n",
    "        ########################### COMPLETAR ###########################\n",
    "        # Para cada capa, resetea los gradientes\n",
    "            \n",
    "        ######################### FIN COMPLETAR #########################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb7f784-c3c7-4069-8980-255309085458",
   "metadata": {},
   "source": [
    "### Stocastic-Gradient Descent (Optimizador)\n",
    "\n",
    "Implementamos un optimizador (SGD) para actualizar los valores de los parámetros.\n",
    "\n",
    "Este optimizador, tendrá, al menos, un método llamado `step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1da810-f40d-4f1b-ba08-a9357e6138ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self, model):\n",
    "        ########################### COMPLETAR ###########################\n",
    "        # Para cada capa con parámetros\n",
    "            # Para cada conjunto de parámetros de la capa\n",
    "                # Para cada gradiente (de cada ejemplo) de cada parámetro\n",
    "                    # Promedio de los gradientes\n",
    "                # Actualización con descenso de gradiente\n",
    "        \n",
    "        ######################### FIN COMPLETAR #########################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e43a837-753b-4fed-ba67-5240bea95987",
   "metadata": {},
   "source": [
    "## **[Ejercicio 3]**\n",
    "\n",
    "Entrena con los datos propuestos, una MLP en regresión."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac121bb2-4564-45a7-9ad0-ab9d5b2e83be",
   "metadata": {},
   "source": [
    "Generamos unos datos a partir de la superficie siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da63701-4914-4423-94ec-a1b76dc2b8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Crear un grid uniforme en el espacio de entrada\n",
    "grid_size = 50  # Tamaño del grid para un muestreo más denso\n",
    "x1 = np.linspace(0, 1, grid_size)\n",
    "x2 = np.linspace(0, 1, grid_size)\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "\n",
    "# Calcular y_true para cada punto en el grid sin ruido\n",
    "Y_org = np.sin(2 * np.pi * X1) + np.cos(2 * np.pi * X2)\n",
    "\n",
    "# Crear la gráfica de superficie\n",
    "fig = go.Figure(data=[go.Surface(\n",
    "    x=X1, \n",
    "    y=X2, \n",
    "    z=Y_org,\n",
    "    colorscale='Inferno'\n",
    ")])\n",
    "\n",
    "# Configuración del layout\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title='X1', \n",
    "        yaxis_title='X2', \n",
    "        aspectmode='cube',  # Asegura escalado igual en todos los ejes\n",
    "        xaxis=dict(backgroundcolor='rgb(230, 230, 230)', gridcolor='white', showbackground=True),  # Fondo gris claro\n",
    "        yaxis=dict(backgroundcolor='rgb(230, 230, 230)', gridcolor='white', showbackground=True),  # Fondo gris claro\n",
    "        zaxis=dict(backgroundcolor='rgb(230, 230, 230)', gridcolor='white', showbackground=True)   # Fondo gris claro\n",
    "    ),\n",
    "    width=800,  # Aumenta el tamaño de la figura\n",
    "    height=800,  # Aumenta el tamaño de la figura\n",
    "    margin=dict(l=0, r=0, b=0, t=50),  # Reduce los márgenes\n",
    "    title=\"Superficie de la función original de la que se obtienen muestras ruidosas\",  # Añade el título\n",
    "    title_font=dict(size=20, family='Arial, sans-serif'),  # Personaliza la fuente del título\n",
    "    scene_camera=dict(eye=dict(x=1.5, y=1.5, z=1.5)),  # Ángulo de cámara predeterminado para mejor visualización\n",
    ")\n",
    "\n",
    "# Mostrar la gráfica\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee44eb4e-e89d-4b60-a3d2-470bd5488c84",
   "metadata": {},
   "source": [
    "Dado el modelo anterior, generamos unas muestras de entrenamiento ruidosas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baa4b54-8822-45ec-a623-a945666e7e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# Parámetros de entrada y salida\n",
    "input_dim = 2   # Dimensiones de entrada\n",
    "output_dim = 1  # Dimensiones de salida\n",
    "\n",
    "# Generación de datos de ejemplo\n",
    "n_samples = 250\n",
    "X_train = np.random.rand(n_samples, input_dim)  # Entradas aleatorias\n",
    "y_train = np.sin(2 * np.pi * X_train[:,0:1]) + np.cos(2 * np.pi * X_train[:,1:]) + 0.1 * np.random.randn(n_samples, output_dim)  # Salidas deseadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b04f35-0aaa-42cb-85ec-a2e62ba95c9e",
   "metadata": {},
   "source": [
    "Entrena, con el `batch_size` que consideres, la `MLP` que has creado anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647a45d7-71c1-4aa4-b99d-22f9f32a0c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "########################### COMPLETAR ###########################\n",
    "# Instancia el modelo, la función de pérdida y el optimizador\n",
    "lr = \n",
    "batch_size = \n",
    "\n",
    "model = \n",
    "loss_fn = \n",
    "opt = \n",
    "######################### FIN COMPLETAR #########################\n",
    "\n",
    "losses = []\n",
    "\n",
    "for e in tqdm(range(epochs)): # Para cada época\n",
    "    loss = []\n",
    "    for b in range(n_samples//batch_size): # Para cada batch\n",
    "        # Obten el batch para el SGD\n",
    "        ########################### COMPLETAR ###########################\n",
    "\n",
    "        \n",
    "        x_train_batch = \n",
    "        y_train_batch = \n",
    "        ######################### FIN COMPLETAR #########################\n",
    "\n",
    "        for i in range(len(x_train_batch)): # Para cada ejemplo del batch\n",
    "\n",
    "            ########################### COMPLETAR ###########################\n",
    "            # Forward\n",
    "            y_pred =  # Reshape de cada ejemplo para pasar por el modelo\n",
    "            loss_value = \n",
    "            # Backward\n",
    "\n",
    "            ######################### FIN COMPLETAR #########################\n",
    "\n",
    "            loss.append(loss_value)\n",
    "\n",
    "        ########################### COMPLETAR ###########################\n",
    "        # Descenso de gradiente (optimizador)\n",
    "\n",
    "        # Reset de los gradientes calculados\n",
    "\n",
    "        ######################### FIN COMPLETAR #########################\n",
    "\n",
    "        losses.append(np.mean(loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6fec20-c439-4b8f-bc95-1d27608482c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar la pérdida\n",
    "plt.figure()\n",
    "plt.plot(np.asarray(losses))\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f70a70e-0ff4-4fae-8c0c-6e9b65ff639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "# Lista de predicciones\n",
    "predictions = []\n",
    "\n",
    "# Prediciones de entrenamiento\n",
    "for i in range(len(X_train)):\n",
    "    y_pred = model(X_train[i].reshape(1, -1).T).flatten()[0]  # Ensure single value output\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "# Plots\n",
    "fig = go.Figure()\n",
    "\n",
    "# Plot predicciones\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=X_train[:, 0],  \n",
    "    y=X_train[:, 1],  \n",
    "    z=predictions, \n",
    "    mode='markers',\n",
    "    marker=dict(color='red', size=8, symbol='circle', opacity=0.8),\n",
    "    name='y_pred',  \n",
    "))\n",
    "\n",
    "# Plot y_train\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=X_train[:, 0],  \n",
    "    y=X_train[:, 1],  \n",
    "    z=y_train.flatten(), \n",
    "    mode='markers',\n",
    "    marker=dict(color='blue', size=8, symbol='circle', opacity=0.8),  \n",
    "    name='y_true',  \n",
    "))\n",
    "\n",
    "# Ajuste del layout\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title='X1', \n",
    "        yaxis_title='X2', \n",
    "        aspectmode='cube',  # Ensures equal scaling of all axes\n",
    "        xaxis=dict(backgroundcolor='rgb(230, 230, 230)', gridcolor='white', showbackground=True),  # Light gray background\n",
    "        yaxis=dict(backgroundcolor='rgb(230, 230, 230)', gridcolor='white', showbackground=True),  # Light gray background\n",
    "        zaxis=dict(backgroundcolor='rgb(230, 230, 230)', gridcolor='white', showbackground=True)   # Light gray background\n",
    "    ),\n",
    "    width=800,  # Increase the size of the figure\n",
    "    height=800,  # Increase the size of the figure\n",
    "    margin=dict(l=0, r=0, b=0, t=0),  # Reduce margins\n",
    "    title=\"Predición de la MLP\",  # Add title\n",
    "    title_font=dict(size=20, family='Arial, sans-serif'),  # Customize title font\n",
    "    scene_camera=dict(eye=dict(x=1.5, y=1.5, z=1.5)),  # Set the default camera angle for better visualization\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45165f9-52d2-4720-8892-1e4eb54071bf",
   "metadata": {},
   "source": [
    "## **[Ejercicio 4]**\n",
    "\n",
    "Implementa la clase `L1Loss` y entrena el mismo modelo ahora con esta función de pérdidas. Compara los resultados. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1defdd-849f-4347-9e48-f6c238acdd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1Loss:\n",
    "    def __init__(self):\n",
    "        ########################### COMPLETAR ###########################\n",
    "        self.p = \n",
    "        ######################### FIN COMPLETAR #########################\n",
    "\n",
    "    def __call__(self, y_pred, y_true):\n",
    "        return self.forward(y_pred, y_true)\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        ########################### COMPLETAR ###########################\n",
    "        self.y_pred = \n",
    "        self.y_true = \n",
    "        self.f =    \n",
    "        return self.f\n",
    "        ######################### FIN COMPLETAR #########################\n",
    "\n",
    "    def backward(self, model=None):\n",
    "        ########################### COMPLETAR ###########################\n",
    "        self.df_dx = \n",
    "        \n",
    "        if model is not None:\n",
    "            model.backward(self.df_dx)\n",
    "        ######################### FIN COMPLETAR #########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf5a489-8dda-4da3-875f-95fedc412181",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "########################### COMPLETAR ###########################\n",
    "# Instancia el modelo, la función de pérdida y el optimizador\n",
    "lr = \n",
    "batch_size = \n",
    "\n",
    "model = \n",
    "loss_fn = \n",
    "opt = \n",
    "######################### FIN COMPLETAR #########################\n",
    "\n",
    "losses_l1 = []\n",
    "\n",
    "for e in tqdm(range(epochs)): # Para cada época\n",
    "    loss = []\n",
    "    for b in range(n_samples//batch_size): # Para cada batch\n",
    "        # Obten el batch para el SGD\n",
    "        ########################### COMPLETAR ###########################\n",
    "\n",
    "        \n",
    "        x_train_batch = \n",
    "        y_train_batch = \n",
    "        ######################### FIN COMPLETAR #########################\n",
    "\n",
    "        for i in range(len(x_train_batch)): # Para cada ejemplo del batch\n",
    "\n",
    "            ########################### COMPLETAR ###########################\n",
    "            # Forward\n",
    "            y_pred =  # Reshape de cada ejemplo para pasar por el modelo\n",
    "            loss_value = \n",
    "            # Backward\n",
    "\n",
    "            ######################### FIN COMPLETAR #########################\n",
    "\n",
    "            loss.append(loss_value)\n",
    "\n",
    "        ########################### COMPLETAR ###########################\n",
    "        # Descenso de gradiente (optimizador)\n",
    "\n",
    "        # Reset de los gradientes calculados\n",
    "\n",
    "        ######################### FIN COMPLETAR #########################\n",
    "\n",
    "        losses_l1.append(np.mean(loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66567946-1caf-40ee-9d23-0a8153177920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar la pérdida\n",
    "plt.figure()\n",
    "plt.plot(np.asarray(losses))\n",
    "plt.plot(np.asarray(losses_l1))\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.title('Training Loss')\n",
    "plt.legend(['MSE', 'L1'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c108eca-fba0-4816-8c57-7a21b6dd4b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "# Lista de predicciones\n",
    "predictions = []\n",
    "\n",
    "# Prediciones de entrenamiento\n",
    "for i in range(len(X_train)):\n",
    "    y_pred = model(X_train[i].reshape(1, -1).T).flatten()[0]  # Ensure single value output\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "# Plots\n",
    "fig = go.Figure()\n",
    "\n",
    "# Plot predicciones\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=X_train[:, 0],  \n",
    "    y=X_train[:, 1],  \n",
    "    z=predictions, \n",
    "    mode='markers',\n",
    "    marker=dict(color='red', size=8, symbol='circle', opacity=0.8),\n",
    "    name='y_pred',  \n",
    "))\n",
    "\n",
    "# Plot y_train\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=X_train[:, 0],  \n",
    "    y=X_train[:, 1],  \n",
    "    z=y_train.flatten(), \n",
    "    mode='markers',\n",
    "    marker=dict(color='blue', size=8, symbol='circle', opacity=0.8),  \n",
    "    name='y_true',  \n",
    "))\n",
    "\n",
    "# Ajuste del layout\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title='X1', \n",
    "        yaxis_title='X2', \n",
    "        aspectmode='cube',  # Ensures equal scaling of all axes\n",
    "        xaxis=dict(backgroundcolor='rgb(230, 230, 230)', gridcolor='white', showbackground=True),  # Light gray background\n",
    "        yaxis=dict(backgroundcolor='rgb(230, 230, 230)', gridcolor='white', showbackground=True),  # Light gray background\n",
    "        zaxis=dict(backgroundcolor='rgb(230, 230, 230)', gridcolor='white', showbackground=True)   # Light gray background\n",
    "    ),\n",
    "    width=800,  # Increase the size of the figure\n",
    "    height=800,  # Increase the size of the figure\n",
    "    margin=dict(l=0, r=0, b=0, t=0),  # Reduce margins\n",
    "    title=\"Predición de la MLP\",  # Add title\n",
    "    title_font=dict(size=20, family='Arial, sans-serif'),  # Customize title font\n",
    "    scene_camera=dict(eye=dict(x=1.5, y=1.5, z=1.5)),  # Set the default camera angle for better visualization\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6baf623-9f03-4eb4-a2ef-b11708cd6821",
   "metadata": {},
   "source": [
    "## **[Ejercicio 5]** \n",
    "\n",
    "Entrena el modelo anterior adaptándolo al `breast cancer dataset`que ya conoces usando `MSELoss` y `L1Loss`. De nuevo, compara los resultados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de11ec72-a4d3-43b9-ab56-c3a8c232f58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "########################### COMPLETAR ###########################\n",
    "# Cargar el dataset\n",
    "\n",
    "\n",
    "# Dividir el dataset en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = \n",
    "\n",
    "\n",
    "######################### FIN COMPLETAR #########################\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "# Añade un método `predict` para contemplar la predicción de varios ejemplos\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_dim, output_dim, std=1):\n",
    "        ########################### COMPLETAR ###########################\n",
    "        F1 =  # Capa lineal con 128 neuronas de salida\n",
    "        Act1 = # Capa de activación\n",
    "        F2 =  # Capa de salida\n",
    "        self.layers = [F1, Act1, F2]\n",
    "        ######################### FIN COMPLETAR #########################\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ########################### COMPLETAR ###########################\n",
    "        # Forward de la red\n",
    "        \n",
    "        return \n",
    "        ######################### FIN COMPLETAR #########################\n",
    "\n",
    "    def backward(self, dL_df):\n",
    "        ########################### COMPLETAR ###########################\n",
    "        # Backward de la red\n",
    "        \n",
    "        ######################### FIN COMPLETAR #########################\n",
    "\n",
    "    def zero_grads(self):\n",
    "        ########################### COMPLETAR ###########################\n",
    "        # Para cada capa, resetea los gradientes\n",
    "            \n",
    "        ######################### FIN COMPLETAR #########################\n",
    "\n",
    "    def predict(self, x):\n",
    "        ########################### COMPLETAR ###########################\n",
    "\n",
    "        # Para cada ejemplo del batch\n",
    "\n",
    "        return \n",
    "        ######################### FIN COMPLETAR #########################\n",
    "\n",
    "\n",
    "epochs = 500\n",
    "\n",
    "########################### COMPLETAR ###########################\n",
    "# Instancia el modelo, la función de pérdida y el optimizador\n",
    "lr = \n",
    "batch_size = \n",
    "\n",
    "model = \n",
    "loss_fn = \n",
    "opt = \n",
    "######################### FIN COMPLETAR #########################\n",
    "losses = []\n",
    "\n",
    "for e in tqdm(range(epochs)):\n",
    "    loss = []\n",
    "    for b in range(n_samples//batch_size):\n",
    "        \n",
    "        ########################### COMPLETAR ###########################\n",
    "\n",
    "        \n",
    "        ######################### FIN COMPLETAR #########################\n",
    "\n",
    "# Visualizar la pérdida\n",
    "plt.plot(np.asarray(losses))\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.title('Pérdida de entrenamiento')\n",
    "plt.show()\n",
    "\n",
    "# Hacer predicciones en el conjunto de prueba\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "y_pred_class = np.where(y_pred_test > 0.5, 1, 0)\n",
    "# Calcular precisión en el conjunto de prueba\n",
    "accuracy = np.mean(y_pred_class == y_test)\n",
    "print(f\"Precisión en el conjunto de test: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2821639-2cde-46de-a20d-0d72d0c94501",
   "metadata": {},
   "source": [
    "## **[Ejercicio 6]** \n",
    "\n",
    "Entrena el modelo anterior adaptándolo al `breast cancer dataset`que ya conoces implementando previamente una función de pérdida de clasificación, por ejemplo, la *binary cross entropy*  `BXELoss`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
