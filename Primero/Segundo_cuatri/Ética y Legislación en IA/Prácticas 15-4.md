**Apartado de Deliveroo**

Un modelo organizativo de la plataforma Deliveroo se basa en la reputación digital del rider, conductor que se ofrece a realizar el servicio. Privilegia al totalmente disponible, y penaliza a aquel que habiendo reservado una sesión de trabajo, la canceló posteriormente. 

Este algoritmo llamado Frank valora como elemento de preferencia para la reserva de sesiones sucesivas, así como para elegir mejores franjas horarias para el servicio de entrega, el hecho de no cancelar aunque sea anticipado, la reserva realizada por el rider. Frank entonces excluye lentamente del ciclo productivo al repartidor que no asegura la disponibilidad. 

- ¿Considera que Frank puede discriminar? Se puede considerar una discriminación puesto que se excluye y por tanto se discrimina a aquel que no tenga una disponibilidad plena. El sistema por lo tanto no tiene en cuenta los motivos que pueden llevar a un individuo a anteponer sus necesidades antes que el trabajo. Así, dentro de la propia autonomía que tiene cada trabajador "autónomo" dentro de Deliveroo, se le excluye por el simple hecho de no vivir trabajando todos los días las 24 horas del mismo.
- ¿Es irrelevante el motivo por el que un rider cancela una sesión de trabajo? No, pues es un claro indicador de lo que le pasa al trabajador. Por tanto, es responsabilidad del sistema el ser consciente de los diferentes motivos por los que una persona puede faltar, ya que no siempre tenemos buenos días, semanas o meses y que ello impacto de forma negativa a tu ganancia económica no fomenta un espacio de trabajo adecuado.

**Apartado de Facebook**

Según el Departamento de Vivienda y Desarrollo Urbano de los Estados Unidos, resulta que Facebook infringió la ley al permitir que los anuncios de viviendas se orientasen en función de la raza, el género y la religión de cada individuo. 

- ¿Considera que este tipo de algoritmos que analizan grandes cantidades de datos, pueden replicar y perpetuar prejuicios existentes de raza, género o religión? Por supuesto, pues aprenden a partir de los estereotipos pasados, creando y perpetuando de esta manera aún más prejuicios. Esto por lo tanto es un sesgo terrible que atenta contra la equidad en todos sus aspectos. Cabe destacar que el problema no es el modelo de IA, pues este solo se abstiene a repetir los patrones que aprendió a base de los datos que se le entregaron. Ahí reside el problema, en los datos que alimentan a nuestro modelo, que ha de filtrarse en pos de evitar la replicación y continuación de estos prejuicios tan dañinos. 